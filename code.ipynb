{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "from os import listdir, getcwd\n",
    "from os.path import join\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "from lib import unet\n",
    "from lib import generator\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = join(getcwd(), 'data')\n",
    "TRAIN_DIR = join(DATA_DIR, 'train')\n",
    "MASK_DIR = join(DATA_DIR, 'train_masks')\n",
    "TRAIN_IMAGE_LIST = sorted(listdir(TRAIN_DIR))\n",
    "MASK_IMAGE_LIST = sorted(listdir(MASK_DIR))\n",
    "IMG_WIDTH = 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "train_images, test_images, train_masks, test_masks = train_test_split(TRAIN_IMAGE_LIST, \n",
    "                                                                      MASK_IMAGE_LIST, \n",
    "                                                                      test_size=0.10000)\n",
    "train_images, validation_images, train_masks, validation_masks = train_test_split(train_images,\n",
    "                                                                                  train_masks,\n",
    "                                                                                  test_size=0.11111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentations - 320px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator.image_generator(TRAIN_DIR, \n",
    "                                            MASK_DIR, \n",
    "                                            train_images, \n",
    "                                            train_masks, \n",
    "                                            batch_size = 2, \n",
    "                                            img_dim = [int((1280 / 1918) * IMG_WIDTH), IMG_WIDTH], \n",
    "                                            num_colors = 8)\n",
    "\n",
    "validation_generator = generator.image_generator(TRAIN_DIR, \n",
    "                                                 MASK_DIR, \n",
    "                                                 validation_images, \n",
    "                                                 validation_masks, \n",
    "                                                 batch_size = 2, \n",
    "                                                 img_dim = [int((1280 / 1918) * IMG_WIDTH), IMG_WIDTH], \n",
    "                                                 num_colors = 8)\n",
    "\n",
    "test_generator = generator.image_generator(TRAIN_DIR, \n",
    "                                           MASK_DIR, \n",
    "                                           test_images, \n",
    "                                           test_masks, \n",
    "                                           batch_size = 2, \n",
    "                                           img_dim = [int((1280 / 1918) * IMG_WIDTH), IMG_WIDTH], \n",
    "                                           num_colors = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet.get_unet_model([IMG_WIDTH, IMG_WIDTH, 3])\n",
    "\n",
    "model.compile(optimizer=Adam(1e-4), \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[unet.dice_coef])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('best-fit-model-aug-320.hdf5',\n",
    "                             monitor='val_dice_coef',\n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  27/2035 [..............................] - ETA: 6713s - loss: 0.6518 - dice_coef: 0.0054"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(train_generator,\n",
    "                           steps_per_epoch = 2035,\n",
    "                           epochs = 10,\n",
    "                           validation_data = validation_generator,\n",
    "                           validation_steps = 252,\n",
    "                           callbacks=[checkpoint])\n",
    "model.save('hist-aug-320.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(hist.history, open('hist-aug-320.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation - 640px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator.image_generator(TRAIN_DIR, \n",
    "                                            MASK_DIR, \n",
    "                                            train_images, \n",
    "                                            train_masks, \n",
    "                                            batch_size = 2, \n",
    "                                            img_dim = [int((1280 / 1918) * IMG_WIDTH), IMG_WIDTH], \n",
    "                                            num_colors = 8)\n",
    "\n",
    "validation_generator = generator.image_generator(TRAIN_DIR, \n",
    "                                                 MASK_DIR, \n",
    "                                                 validation_images, \n",
    "                                                 validation_masks, \n",
    "                                                 batch_size = 2, \n",
    "                                                 img_dim = [int((1280 / 1918) * IMG_WIDTH), IMG_WIDTH], \n",
    "                                                 num_colors = 8)\n",
    "\n",
    "test_generator = generator.image_generator(TRAIN_DIR, \n",
    "                                           MASK_DIR, \n",
    "                                           test_images, \n",
    "                                           test_masks, \n",
    "                                           batch_size = 2, \n",
    "                                           img_dim = [int((1280 / 1918) * IMG_WIDTH), IMG_WIDTH], \n",
    "                                           num_colors = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet.get_unet_model([IMG_WIDTH, IMG_WIDTH, 3])\n",
    "\n",
    "model.compile(optimizer=Adam(1e-4), \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[unet.dice_coef])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('best-fit-model-aug-640.hdf5',\n",
    "                             monitor='val_dice_coef',\n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2034/2035 [============================>.] - ETA: 3s - loss: 0.1040 - dice_coef: 0.7965Epoch 00000: val_dice_coef improved from -inf to 0.97465, saving model to best-fit-model-aug-640.hdf5\n",
      "2035/2035 [==============================] - 6785s - loss: 0.1040 - dice_coef: 0.7966 - val_loss: 0.0223 - val_dice_coef: 0.9747\n",
      "Epoch 2/10\n",
      "2034/2035 [============================>.] - ETA: 3s - loss: 0.1073 - dice_coef: 0.8539Epoch 00001: val_dice_coef did not improve\n",
      "2035/2035 [==============================] - 6724s - loss: 0.1073 - dice_coef: 0.8539 - val_loss: 0.0393 - val_dice_coef: 0.9573\n",
      "Epoch 3/10\n",
      "2034/2035 [============================>.] - ETA: 3s - loss: 0.0429 - dice_coef: 0.9523Epoch 00002: val_dice_coef did not improve\n",
      "2035/2035 [==============================] - 6709s - loss: 0.0429 - dice_coef: 0.9523 - val_loss: 0.0299 - val_dice_coef: 0.9646\n",
      "Epoch 4/10\n",
      "2034/2035 [============================>.] - ETA: 3s - loss: 0.0251 - dice_coef: 0.9722Epoch 00003: val_dice_coef improved from 0.97465 to 0.98177, saving model to best-fit-model-aug-640.hdf5\n",
      "2035/2035 [==============================] - 6709s - loss: 0.0251 - dice_coef: 0.9722 - val_loss: 0.0168 - val_dice_coef: 0.9818\n",
      "Epoch 5/10\n",
      "2034/2035 [============================>.] - ETA: 3s - loss: 0.0217 - dice_coef: 0.9749Epoch 00004: val_dice_coef did not improve\n",
      "2035/2035 [==============================] - 6750s - loss: 0.0217 - dice_coef: 0.9749 - val_loss: 0.1360 - val_dice_coef: 0.8182\n",
      "Epoch 6/10\n",
      "2034/2035 [============================>.] - ETA: 3s - loss: 0.0429 - dice_coef: 0.9421Epoch 00005: val_dice_coef did not improve\n",
      "2035/2035 [==============================] - 6752s - loss: 0.0429 - dice_coef: 0.9421 - val_loss: 0.0196 - val_dice_coef: 0.9788\n",
      "Epoch 7/10\n",
      "2034/2035 [============================>.] - ETA: 3s - loss: 0.0127 - dice_coef: 0.9852Epoch 00006: val_dice_coef improved from 0.98177 to 0.98320, saving model to best-fit-model-aug-640.hdf5\n",
      "2035/2035 [==============================] - 6725s - loss: 0.0127 - dice_coef: 0.9852 - val_loss: 0.0146 - val_dice_coef: 0.9832\n",
      "Epoch 8/10\n",
      "2034/2035 [============================>.] - ETA: 3s - loss: 0.0157 - dice_coef: 0.9827Epoch 00007: val_dice_coef improved from 0.98320 to 0.98840, saving model to best-fit-model-aug-640.hdf5\n",
      "2035/2035 [==============================] - 6683s - loss: 0.0157 - dice_coef: 0.9827 - val_loss: 0.0103 - val_dice_coef: 0.9884\n",
      "Epoch 9/10\n",
      "2034/2035 [============================>.] - ETA: 3s - loss: 0.0161 - dice_coef: 0.9823Epoch 00008: val_dice_coef did not improve\n",
      "2035/2035 [==============================] - 6686s - loss: 0.0160 - dice_coef: 0.9823 - val_loss: 0.0116 - val_dice_coef: 0.9866\n",
      "Epoch 10/10\n",
      "2034/2035 [============================>.] - ETA: 3s - loss: 0.0121 - dice_coef: 0.9867Epoch 00009: val_dice_coef improved from 0.98840 to 0.98925, saving model to best-fit-model-aug-640.hdf5\n",
      "2035/2035 [==============================] - 6677s - loss: 0.0121 - dice_coef: 0.9867 - val_loss: 0.0089 - val_dice_coef: 0.9892\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(train_generator,\n",
    "                           steps_per_epoch = 2035,\n",
    "                           epochs = 10,\n",
    "                           validation_data = validation_generator,\n",
    "                           validation_steps = 252,\n",
    "                           callbacks=[checkpoint])\n",
    "model.save('hist-aug-640.h5')\n",
    "pickle.dump(hist.history, open('hist-aug-640.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Augmentation - 320px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator.image_generator_no_aug(TRAIN_DIR, \n",
    "                                            MASK_DIR, \n",
    "                                            train_images, \n",
    "                                            train_masks, \n",
    "                                            batch_size = 2, \n",
    "                                            img_dim = [int((1280 / 1918) * IMG_WIDTH), IMG_WIDTH], \n",
    "                                            num_colors = 8)\n",
    "\n",
    "validation_generator = generator.image_generator_no_aug(TRAIN_DIR, \n",
    "                                                 MASK_DIR, \n",
    "                                                 validation_images, \n",
    "                                                 validation_masks, \n",
    "                                                 batch_size = 2, \n",
    "                                                 img_dim = [int((1280 / 1918) * IMG_WIDTH), IMG_WIDTH], \n",
    "                                                 num_colors = 8)\n",
    "\n",
    "test_generator = generator.image_generator_no_aug(TRAIN_DIR, \n",
    "                                           MASK_DIR, \n",
    "                                           test_images, \n",
    "                                           test_masks, \n",
    "                                           batch_size = 2, \n",
    "                                           img_dim = [int((1280 / 1918) * IMG_WIDTH), IMG_WIDTH], \n",
    "                                           num_colors = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet.get_unet_model([IMG_WIDTH, IMG_WIDTH, 3])\n",
    "\n",
    "model.compile(optimizer=Adam(1e-4), \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[unet.dice_coef])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('best-fit-model-no-aug-320.hdf5',\n",
    "                             monitor='val_dice_coef',\n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit_generator(train_generator,\n",
    "                           steps_per_epoch = 2035,\n",
    "                           epochs = 10,\n",
    "                           validation_data = validation_generator,\n",
    "                           validation_steps = 252,\n",
    "                           callbacks=[checkpoint])\n",
    "model.save('hist-no-aug-320.h5')\n",
    "pickle.dump(hist.history, open('hist-no-aug-320.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Augmentation - 640px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator.image_generator_no_aug(TRAIN_DIR, \n",
    "                                            MASK_DIR, \n",
    "                                            train_images, \n",
    "                                            train_masks, \n",
    "                                            batch_size = 2, \n",
    "                                            img_dim = [int((1280 / 1918) * IMG_WIDTH), IMG_WIDTH], \n",
    "                                            num_colors = 8)\n",
    "\n",
    "validation_generator = generator.image_generator_no_aug(TRAIN_DIR, \n",
    "                                                 MASK_DIR, \n",
    "                                                 validation_images, \n",
    "                                                 validation_masks, \n",
    "                                                 batch_size = 2, \n",
    "                                                 img_dim = [int((1280 / 1918) * IMG_WIDTH), IMG_WIDTH], \n",
    "                                                 num_colors = 8)\n",
    "\n",
    "test_generator = generator.image_generator_no_aug(TRAIN_DIR, \n",
    "                                           MASK_DIR, \n",
    "                                           test_images, \n",
    "                                           test_masks, \n",
    "                                           batch_size = 2, \n",
    "                                           img_dim = [int((1280 / 1918) * IMG_WIDTH), IMG_WIDTH], \n",
    "                                           num_colors = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet.get_unet_model([IMG_WIDTH, IMG_WIDTH, 3])\n",
    "\n",
    "model.compile(optimizer=Adam(1e-4), \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[unet.dice_coef])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('best-fit-model-no-aug-640.hdf5',\n",
    "                             monitor='val_dice_coef',\n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2034/2035 [============================>.] - ETA: 3s - loss: 0.1208 - dice_coef: 0.7713Epoch 00000: val_dice_coef improved from -inf to 0.92719, saving model to best-fit-model-no-aug-640.hdf5\n",
      "2035/2035 [==============================] - 6835s - loss: 0.1207 - dice_coef: 0.7713 - val_loss: 0.0835 - val_dice_coef: 0.9272\n",
      "Epoch 2/10\n",
      "2034/2035 [============================>.] - ETA: 3s - loss: 0.0412 - dice_coef: 0.9499Epoch 00001: val_dice_coef did not improve\n",
      "2035/2035 [==============================] - 6714s - loss: 0.0413 - dice_coef: 0.9498 - val_loss: 0.1586 - val_dice_coef: 0.8044\n",
      "Epoch 3/10\n",
      "2034/2035 [============================>.] - ETA: 3s - loss: 0.0307 - dice_coef: 0.9658Epoch 00002: val_dice_coef improved from 0.92719 to 0.98120, saving model to best-fit-model-no-aug-640.hdf5\n",
      "2035/2035 [==============================] - 6678s - loss: 0.0307 - dice_coef: 0.9658 - val_loss: 0.0153 - val_dice_coef: 0.9812\n",
      "Epoch 4/10\n",
      "2034/2035 [============================>.] - ETA: 3s - loss: 0.0395 - dice_coef: 0.9482Epoch 00003: val_dice_coef did not improve\n",
      "2035/2035 [==============================] - 6701s - loss: 0.0395 - dice_coef: 0.9483 - val_loss: 0.0181 - val_dice_coef: 0.9773\n",
      "Epoch 5/10\n",
      "2034/2035 [============================>.] - ETA: 3s - loss: 0.0240 - dice_coef: 0.9745Epoch 00004: val_dice_coef did not improve\n",
      "2035/2035 [==============================] - 6692s - loss: 0.0240 - dice_coef: 0.9745 - val_loss: 0.0170 - val_dice_coef: 0.9795\n",
      "Epoch 6/10\n",
      "2034/2035 [============================>.] - ETA: 3s - loss: 0.0157 - dice_coef: 0.9822Epoch 00005: val_dice_coef improved from 0.98120 to 0.98677, saving model to best-fit-model-no-aug-640.hdf5\n",
      "2035/2035 [==============================] - 6672s - loss: 0.0157 - dice_coef: 0.9822 - val_loss: 0.0110 - val_dice_coef: 0.9868\n",
      "Epoch 7/10\n",
      "2034/2035 [============================>.] - ETA: 3s - loss: 0.0252 - dice_coef: 0.9761Epoch 00006: val_dice_coef did not improve\n",
      "2035/2035 [==============================] - 6670s - loss: 0.0252 - dice_coef: 0.9761 - val_loss: 0.0175 - val_dice_coef: 0.9837\n",
      "Epoch 8/10\n",
      "2034/2035 [============================>.] - ETA: 3s - loss: 0.0128 - dice_coef: 0.9856Epoch 00007: val_dice_coef did not improve\n",
      "2035/2035 [==============================] - 6663s - loss: 0.0128 - dice_coef: 0.9856 - val_loss: 0.0157 - val_dice_coef: 0.9829\n",
      "Epoch 9/10\n",
      "2034/2035 [============================>.] - ETA: 3s - loss: 0.0185 - dice_coef: 0.9809Epoch 00008: val_dice_coef improved from 0.98677 to 0.98751, saving model to best-fit-model-no-aug-640.hdf5\n",
      "2035/2035 [==============================] - 6668s - loss: 0.0185 - dice_coef: 0.9809 - val_loss: 0.0111 - val_dice_coef: 0.9875\n",
      "Epoch 10/10\n",
      "2034/2035 [============================>.] - ETA: 3s - loss: 0.0149 - dice_coef: 0.9839Epoch 00009: val_dice_coef did not improve\n",
      "2035/2035 [==============================] - 6668s - loss: 0.0149 - dice_coef: 0.9839 - val_loss: 0.0112 - val_dice_coef: 0.9875\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(train_generator,\n",
    "                           steps_per_epoch = 2035,\n",
    "                           epochs = 10,\n",
    "                           validation_data = validation_generator,\n",
    "                           validation_steps = 252,\n",
    "                           callbacks=[checkpoint])\n",
    "model.save('hist-no-aug-640.h5')\n",
    "pickle.dump(hist.history, open('hist-no-aug-640.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/\n",
      "models/hist-aug-640.pkl\n",
      "models/hist-aug-320.pkl\n",
      "models/best-fit-model-aug-320.hdf5\n",
      "models/best-fit-model-aug-640.hdf5\n",
      "models/hist-no-aug-640.pkl\n",
      "models/hist-no-aug-320.h5\n",
      "models/hist-aug-320.h5\n",
      "models/hist-no-aug-640.h5\n",
      "models/best-fit-model-no-aug-640.hdf5\n",
      "models/best-fit-model-no-aug-320.hdf5\n",
      "models/hist-no-aug-320.pkl\n",
      "models/hist-aug-640.h5\n"
     ]
    }
   ],
   "source": [
    "!tar -zcvf models.tar.gz models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 320\n",
    "test_320_gen = generator.image_generator_no_aug(TRAIN_DIR, \n",
    "                                           MASK_DIR, \n",
    "                                           test_images, \n",
    "                                           test_masks, \n",
    "                                           batch_size = 5, \n",
    "                                           img_dim = [int((1280 / 1918) * IMG_WIDTH), IMG_WIDTH], \n",
    "                                           num_colors = 8)\n",
    "IMG_WIDTH = 640\n",
    "test_640_gen = generator.image_generator_no_aug(TRAIN_DIR, \n",
    "                                           MASK_DIR, \n",
    "                                           test_images, \n",
    "                                           test_masks, \n",
    "                                           batch_size = 2, \n",
    "                                           img_dim = [int((1280 / 1918) * IMG_WIDTH), IMG_WIDTH], \n",
    "                                           num_colors = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model_no_aug_320 = load_model('./models/hist-no-aug-320.h5')\n",
    "model_no_aug_640 = load_model('./models/hist-no-aug-640.h5')\n",
    "model_aug_320 = load_model('./models/hist-aug-320.h5')\n",
    "model_aug_640 = load_model('./models/hist-aug-640.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_no_aug_320 = model_no_aug_320.evaluate_generator(test_320_gen, steps=102)\n",
    "eval_aug_320 = model_aug_320.evaluate_generator(test_320_gen, steps=102)\n",
    "eval_no_aug_640 = model_no_aug_640.evaluate_generator(test_640_gen, steps=252)\n",
    "eval_aug_640 = model_aug_640.evaluate_generator(test_640_gen, steps=252)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "nteract": {
   "version": "0.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
